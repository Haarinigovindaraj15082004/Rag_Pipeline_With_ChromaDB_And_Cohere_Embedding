# ğŸ“š RAG Pipeline with ChromaDB, Cohere & Groq

This project implements a **Retrieval-Augmented Generation (RAG)** system that extracts knowledge from structured (CSV, Excel) and unstructured (PDF, DOCX, TXT) files, stores it in **ChromaDB**, and answers user questions using **semantic search** and **Groqâ€™s Llama 3 model**.

---

## ğŸš€ Features
- Extracts text from:
  - **Unstructured files** â†’ PDF, DOCX, TXT (via Unstructured.io)
  - **Structured files** â†’ CSV, Excel (via Pandas)
- **Deduplication** â†’ Prevents storing duplicate chunks using MD5 hashing.
- **Semantic Search** â†’ Finds the most relevant chunks using Cohere embeddings + ChromaDB.
- **LLM Integration** â†’ Uses Groq (Llama 3) to generate answers only from retrieved context.
- **User Input Querying** â†’ Ask any question from ingested documents.

---

## âš™ï¸ Tech Stack
- **Python 3.13**
- **ChromaDB (Cloud/Local)** â†’ Vector database
- **Cohere API** â†’ Embeddings
- **Groq API** â†’ LLM responses
- **Unstructured.io** â†’ Document parsing
- **Pandas** â†’ Structured data handling

---

## ğŸ“‚ Project Structure
```

rag-project/
â”‚â”€â”€ rag\_app.py         # Main RAG pipeline
â”‚â”€â”€ requirements.txt   # Dependencies
â”‚â”€â”€ .gitignore         # Ignore env/venv/secrets
â”‚â”€â”€ .env               # API keys (not committed)
â”‚â”€â”€ iv\_list.pdf        # Sample file to ingest

````

---

## ğŸ”‘ Setup

1. Clone repo:
   ```bash
   git clone https://github.com/<your-username>/rag-project.git
   cd rag-project
````

2. Create virtual environment:

   ```bash
   python -m venv venv
   source venv/bin/activate   # Mac/Linux
   venv\Scripts\activate      # Windows
   ```

3. Install dependencies:

   ```bash
   pip install -r requirements.txt
   ```

4. Add `.env` file:

   ```
   COHERE_API_KEY=your_cohere_key
   GROQ_API_KEY=your_groq_key
   CHROMA_TENANT=your_chroma_tenant
   CHROMA_DATABASE=your_chroma_database
   CHROMA_API_KEY=your_chroma_api_key
   ```

---

## â–¶ï¸ Usage

1. Ingest documents:

   ```bash
   python rag_app.py
   ```

2. Enter a query:

   ```
   â“ Ask your question: What are the main findings in the report?
   ```

3. Get an AI-generated answer based only on your documents:

   ```
   ğŸ’¡ Answer: The report highlights an improvement in customer satisfaction during Q2 due to better service response times.
   ```

---

## ğŸ“Š How It Works

1. **Extraction** â†’ Text is extracted & chunked.
2. **Embedding** â†’ Each chunk is embedded using Cohere.
3. **Storage** â†’ Stored in ChromaDB with metadata & hash.
4. **Querying** â†’ User query is embedded, ChromaDB retrieves top-k similar chunks.
5. **Generation** â†’ Groq (Llama 3) uses retrieved chunks to answer.

---

## ğŸ›¡ï¸ Notes

* `.env` file is ignored in Git (`.gitignore`) â†’ never push API keys.
* Supports **both structured & unstructured data**.
* Default retrieval: **Top-3 results (k=3)**.

---

## âœ¨ Example

Input:

```
â“ Ask your question: Why did customer satisfaction improve in Q2?
```

Output:

```
ğŸ’¡ Answer: Customer satisfaction improved in Q2 due to faster service response times and proactive customer engagement.
```

---
